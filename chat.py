import streamlit as st
import os

from dotenv import load_dotenv
from langchain import hub
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone

st.set_page_config(page_title='ì „ì„¸ì‚¬ê¸°í”¼í•´ ìƒë‹´ ì±—ë´‡', page_icon='ğŸ¤–')
st.title('ğŸ¤–ì „ì„¸ì‚¬ê¸°í”¼í•´ ìƒë‹´ ì±—ë´‡')

if 'message_list' not in st.session_state:
    st.session_state.message_list = []

# for msg in st.session_state.message_list:
#     with st.chat_message(msg['role']):
#         st.write(msg['content'])

print(f'before: {st.session_state.message_list}')

## ì´ì „ ì±„íŒ… ë‚´ìš© í™”ë©´ì— ì¶œë ¥(í‘œì‹œ)
for message in st.session_state.message_list:
    with st.chat_message(message['role']):
        st.write(message['content'])

## [AI message í•¨ìˆ˜ ì •ì˜] ##############################
def get_ai_meassge(user_meassge):
   
    ## í™˜ê²½ë³€ìˆ˜ ì½ì–´ì˜¤ê¸° ###########################################################
    load_dotenv()
    PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')
    LANGCHAIN_API_KEY = os.getenv('LANGCHAIN_API_KEY')

    ## ì„ë² ë”© -> ë²¡í„° ìŠ¤í† ì–´(ë°ì´í„°ë² ì´ìŠ¤)ì—ì„œ ì¸ë±ìŠ¤ ê°€ì ¸ì˜¤ê¸° #####################
    ## ì„ë² ë”© ëª¨ë¸ ì§€ì •
    embedding = OpenAIEmbeddings(model='text-embedding-3-large')
    pc = Pinecone(api_key=PINECONE_API_KEY)
    index_name = 'law'

    ## ì €ì¥ëœ ì¸ë±ìŠ¤ ê°€ì ¸ì˜¤ê¸° ######################################################
    database = PineconeVectorStore.from_existing_index(
        index_name=index_name,
        embedding=embedding,
    )

    ## RetrievalQA #################################################################
    llm = ChatOpenAI(model='gpt-4o')
    prompt = hub.pull('rlm/rag-prompt')

    def format_docs(docs):
        return '\n\n'.join(doc.page_content for doc in docs)

    qa_chain = (
        {
            'context': database.as_retriever() | format_docs,
            'question': RunnablePassthrough(),
        }
        | prompt
        | llm
        | StrOutputParser()
    )

    # qa_chain.invoke('ì „ì„¸ì‚¬ê¸°í”¼í•´ì ëŒ€ìƒì„ ì•Œë ¤ì£¼ì„¸ìš”.')
    ai_message = qa_chain.invoke(user_meassge)
    return ai_message


## prompt ì°½(ì±„íŒ… ì°½) #################################################################
placeholder='ì „ì„¸ì‚¬ê¸° í”¼í•´ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.'
if user_question := st.chat_input(placeholder=placeholder):
    with st.chat_message('user'):
        ## ì‚¬ìš©ì ë©”ì‹œì§€ í™”ë©´ ì¶œë ¥
        st.write(user_question)
    st.session_state.message_list.append({'role':'user','content':user_question})

    ai_message = get_ai_meassge('user_question')

    with st.chat_message('ai'):
        ## AI ë©”ì‹œì§€ í™”ë©´ ì¶œë ¥
        st.write(ai_message)
    st.session_state.message_list.append({'role':'ai', 'content': ai_message})

print(f'after: {st.session_state.message_list}')

print(user_question)
